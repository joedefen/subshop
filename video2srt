#! /usr/bin/env python3
"""TBD"""
# pylint: disable=invalid-name
# pylint: disable=broad-except
# pylint: disable=consider-using-f-string

import threading
import queue
import sys
import re
import os
import time
import traceback
import tempfile
import multiprocessing
import json
import shlex
import atexit
import argparse

from vosk import Model, KaldiRecognizer, SetLogLevel
from LibSub.SubFixer import Caption
from LibSub import ConfigSubshop
import LibSub.SubShopDirs as ssd
from LibGen.CustLogger import CustLogger as lg

SetLogLevel(-1)


# process = subprocess.Popen(['ffmpeg', '-loglevel', 'quiet', '-i',
    # sys.argv[1], '-ar', str(sample rate) , '-ac', '1', '-f', 's16le', '-'],
    # stdout=subprocess.PIPE)


class VideoToSrt:
    """TBD"""
    SAMPLE_RATE = 16000
    WORDS_PER_SUBTITLE = 7
    subshop_params = ConfigSubshop.get_params()

    model = os.path.join(ssd.model_d, '.vosk-model')

    def __init__(self):
        self.subs = []
        self.thr_cnt = self.get_thread_cnt()
        self.job_cnt = self.thr_cnt * 1
        self.datas = [None] * self.job_cnt
        self.thr_results = [None] * self.job_cnt
        self.chunk_ms = 0
        self.tmpfile = None
        if not os.path.exists(VideoToSrt.model):
            lg.err('Missing model [{}];  download from alphacephei.com/vosk/models',
                    VideoToSrt.model)
            sys.exit (1)
        lg.tr1('init:', 'thr_cnt:', self.thr_cnt, 'model:', VideoToSrt.model)

    @staticmethod
    def get_thread_cnt():
        """Get an appropriate number of threads for dividing the text-to-speech task."""
        count = VideoToSrt.subshop_params.speech_to_text_params.thread_cnt
        if isinstance(count, int) and count >= 1:
            return count

        count = multiprocessing.cpu_count()
        count -= 2 if count >= 8 else 1
        return max(count, 1)


    def make_wav(self, video, stream):
        """TBD"""
        _, self.tmpfile = tempfile.mkstemp(suffix='.wav')
        lg.tr1('video:', video, 'tmpfile:', self.tmpfile)
        atexit.register(self.cleanup, self.tmpfile)

        # FIRST if stream not provided, then find the desired audio
        # stream if any from a line that looks like:
        #        Stream #0:1(eng): Audio: aac (LC), 48000 Hz, stereo, fltp (default)
        if not stream:
            cmd = (r"ffmpeg -i {} 2>&1 | sed -n -e "
                    r"'/.*Stream #\([0-9:]*\)({}): Audio.*/s//\1/p' | head -1").format(
                            shlex.quote(video), self.subshop_params.my_lang3)
            lg.db("pre-cmd:", cmd)
            stream = os.popen(cmd).read().strip()
            if not re.match(r'\d+:\d+$', stream):
                lg.err('\n-------------',
                        f'FAIL: cannot find {self.subshop_params.my_lang3} audio stream', '\n')
                return

        # SECOND: do the extraction...
        cmd = 'ffmpeg -y -nostats -hide_banner -loglevel error'
        cmd += ' -i {} -map {} -ar 16000 -ac 1 {}'.format(
                shlex.quote(video), stream, shlex.quote(self.tmpfile))

        lg.db('cmd:', cmd)
        rv = os.system(cmd)
        exit_code, signal = 0, 0
        if rv:
            exit_code, signal = rv >> 8, rv & 0x0FF
            lg.err('os.system("ffmpeg ..")',
                    'killed by sig {}'.format(signal) if signal
                    else 'returned {}'.format(exit_code))
            if signal or exit_code == 15:
                raise KeyboardInterrupt

        if exit_code or signal:
            rv = 'FAIL [{}]\n'.format(
                    'ffmpeg killed by sig {}'.format(signal) if signal
                    else 'ffmpeg returned {}'.format(exit_code))
            lg.err('\n-------------', rv, '\n')

    @staticmethod
    def cleanup(tmpfile):
        """TBD"""
        try:
            os.unlink(tmpfile)
        except Exception:
            pass

    @staticmethod
    def worker(data, res_queue, slot, job):
        """TBD"""
        model = Model(VideoToSrt.model)
        rec = KaldiRecognizer(model, VideoToSrt.SAMPLE_RATE)
        rec.SetWords(True)
        results = []
        if rec.AcceptWaveform(data):
            results.append(rec.Result())
        results.append(rec.FinalResult())
        res_queue.put((slot, job, results))

    def speech_to_text(self):
        """TBD"""
        def start_next_job(slot):
            nonlocal self, next_job, threads, res_queue
            job, next_job = next_job, next_job + 1
            threads[slot] = threading.Thread(target=VideoToSrt.worker,
                    args=(self.datas[job], res_queue, slot, job))
            threads[slot].start()

        with open(self.tmpfile, 'rb') as reader:
            file_size = os.fstat(reader.fileno()).st_size
            job_size = (file_size // 64 // self.job_cnt) * 64

            for job in range(self.job_cnt):
                self.datas[job] = reader.read(job_size)
            self.chunk_ms = job_size * 1000 // (2 * VideoToSrt.SAMPLE_RATE)

        # Define a few variables including storage for threads and values.
        threads = [None] * self.thr_cnt
        res_queue = queue.Queue()
        # Create, start, and store all of the thread objects.
        next_job = 0
        for slot in range(len(threads)):
            start_next_job(slot)
        # Ensure all threads are done and show the results.
        last_print_sec, done_cnt = -1, 0
        start_time = time.time()
        while done_cnt < self.job_cnt:
            try:
                slot, job, results = res_queue.get(timeout=0.25)
                # sys.stderr.write(f'[{slot}.{job}]')
                # sys.stderr.flush()
                self.thr_results[job] = results
                threads[slot].join()
                done_cnt += 1
                if next_job < self.job_cnt:
                    start_next_job(slot)
            except queue.Empty:
                pass
            elapsed_sec = (int(round(time.time() - start_time)) // 2) * 2
            if elapsed_sec != last_print_sec:
                sys.stderr.write(f'{elapsed_sec}'if elapsed_sec % 10 == 0 else '.')
                last_print_sec = elapsed_sec
                sys.stderr.flush()

        sys.stderr.write('\n')

    def make_subs(self):
        """TBD"""
        max_word_s = round(60/100, 3) # 100 w/min min rate
        # min_word_s = round(60/160, 3) # 160 w/min max rate

        def flush():
            nonlocal self, wds
            if not wds:
                return
            ## IF using SRT...
            ## sub = srt.Subtitle(index=len(self.subs),
                    ## content=" ".join([wd['word'] for wd in wds]),
                    ## start=datetime.timedelta(seconds=wds[0]['start']),
                    ## end=datetime.timedelta(seconds=wds[-1]['end']))
            ## self.subs.append(sub)
            # print('DB: wds[0]:', wds[0])
            cap = Caption().set(beg_s=wds[0]['start'], end_s=wds[-1]['end'],
                    text=" ".join([wd['word'] for wd in wds]), caplist=self.subs)
            lg.tr9('DB: caption:', cap, '#caps:', len(self.subs), vars(cap))
            wds = []

        self.subs = []
        words = [] # assemble the words; adjust offset
        for idx, thr_res in enumerate(self.thr_results):
            offset_s = idx*self.chunk_ms / 1000
            for res in thr_res:
                jres = json.loads(res)
                if not 'result' in jres:
                    continue
                for word in jres['result']:
                    word['start'] += offset_s
                    word['end'] += offset_s
                    words.append(word)

        wds = [] # connect words forming one subtitle
        for word in words:
            if len(wds) >= VideoToSrt.WORDS_PER_SUBTITLE:
                flush()
            elif wds: # flush subtible if new word is not connected tightly
                max_s = max_word_s*len(wds) #, min_word_s*len(wds)
                if not (word['end'] - wds[0]['start']) <= max_s:
                    flush()
            wds.append(word)
        flush()

    def prc_video(self, video, outfile=None, stream=None):
        """TBD"""
        self.make_wav(video, stream=stream)
        self.speech_to_text()
        self.make_subs()
        if not self.subs:
            lg.err('no subs found')
            return False
        srt_guts = Caption.compose(self.subs)
        if outfile:
            sav_stdout = sys.stdout
            with open(outfile, 'w', encoding='utf-8') as outf:
                sys.stdout = outf
                print(srt_guts)
                ## print(srt.compose(self.subs))
                sys.stdout = sav_stdout
        else:
            print(srt_guts)
            ## print(srt.compose(self.subs))

        return True


if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument('-o', '--output',
                help='specify the output file (else uses stdout)')
        parser.add_argument('-s', '--stream', help='audio stream (e.g, "0:2")')
        parser.add_argument('-V', '--log-level', choices=lg.choices,
            default='INFO', help='set logging/verbosity level [dflt=INFO]')
        parser.add_argument('video', nargs=1, default=None,
                help='specify the videofile')
        args = parser.parse_args()
        lg.setup(level=args.log_level)
        lg.tr1('video:', args.video[0], 'out:', args.output, 'stream:', args.stream)
        tool = VideoToSrt()
        retval = tool.prc_video(args.video[0], args.output, args.stream)
        sys.exit(0 if retval else 1)

    except KeyboardInterrupt:
        print("Shutdown requested, so exiting ...")
    except Exception as exc:
        print("exception:", str(exc))
        print(traceback.format_exc())
    sys.exit(0)
